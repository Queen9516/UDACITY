{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Top of document'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle OpenStreetMap Data\n",
    "\n",
    "## Data Wrangling\n",
    "\n",
    "* [Quiz: Parsing CSV Files](#quiz_parsing_csv)\n",
    "    * [Testing Code](#testing_code_1)\n",
    "    * [My Solution: Parsing CSV Files](#my_sol_parsing_csv)\n",
    "    * [Instructor Solution: Parsing CSV Files](#inst_sol_parsing_csv)\n",
    "    * [With CSV Module](#csv_module)\n",
    "* [Intro to XLRD](#intro_xlrd)\n",
    "    * [Quiz: Reading Excel Files](#qz_reading_excel_files)\n",
    "    * [My Solution: Reading Excel Files](#my_sol_reading_excel_files)\n",
    "    * [Test: Reading Excel Files](#test_reading_excel_files)\n",
    "* [Quiz: JSON](#quiz_json)\n",
    "* [Quiz: Using CSV Module](#quiz_csv_mod)\n",
    "    * [My Solution](#my_sol_using_csv_mod)\n",
    "* [Quiz: Excel to CSV](#quiz_excel_csv)\n",
    "    * [My Solution](#my_sol_excel_csv)\n",
    "    * [Instructor Solution](#inst_sol_excel_csv)\n",
    "    * [Test](#test_excel_csv)\n",
    "* [Quiz: Wrangling JSON](#quiz_wrangling_json)\n",
    "    * [My Solution](#my_sol_wrangling_json)\n",
    "    * [Instructor Solution](#ins_sol_wrangling_json)\n",
    "* [Data in More Complex Formats](#data_complex_formats)\n",
    "    * [XML](#data_complex_formats)\n",
    "    * [Quiz: Extracting Data XML](#quiz_extracting_data_xml)\n",
    "    * [Quiz: Handling Attributes XML](#quiz_handling_attributes_xml)\n",
    "    * [Quiz: Using Beautiful Soup](#quiz_beautiful_soup)\n",
    "    * [Quiz: Carrier List](#quiz_carrier_list)\n",
    "    * [Quiz: Airport List](#quiz_airport_list)\n",
    "    * [Quiz: Processing All](#quiz_processing_all)\n",
    "    * [Quiz: Patent Database](#quiz_patent_database)\n",
    "    * [Quiz: Result of Parsing the Datafile](#quiz_result_parsing_datafile)\n",
    "    * [Quiz: Processing Patents](#quiz_processing_patents)\n",
    "* [Data Quality](#data_quality)\n",
    "    * [Quiz: Correcting Validity](#quiz_correcting_validity)\n",
    "    * [Quiz: Auditing Data Quality](#quiz_auditing_data_quality)\n",
    "    * [Quiz: Fixing the Area](#quiz_fixing_area)\n",
    "    * [Quiz: Fixing Name](#quiz_fixing_name)\n",
    "    * [Quiz: Crossfield Auditing](#quiz_crossfield_auditing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='quiz_parsing_csv'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Parsing CSV Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to read the input DATAFILE line by line, and for the first 10 lines (not including the header) split each line on \",\" and then for each line, create a dictionary where the key is the header title of the field, and the value is the value of that field in the row.\n",
    "The function parse_file should return a list of dictionaries, each data line in the file being a single list entry.\n",
    "Field names and values should not contain extra whitespace, like spaces or newline characters.\n",
    "You can use the Python string method strip() to remove the extra whitespace.\n",
    "You have to parse only the first 10 data lines in this exercise, so the returned list should have 10 entries!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='testing_code_1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATADIR = os.getcwd()\n",
    "DATAFILE = \"beatles-diskography.csv\"\n",
    "\n",
    "datafile = os.path.join(DATADIR, DATAFILE)\n",
    "\n",
    "print(datafile)\n",
    "\n",
    "data = []\n",
    "with open(datafile, \"r\") as f:\n",
    "    for line in f:\n",
    "        music = dict()\n",
    "        line.strip('\\n')\n",
    "        words = line.split(',')\n",
    "        music['Title'] = words[0]\n",
    "        music['UK Chart Position'] = words[3]\n",
    "        music['Label'] = words[2]\n",
    "        music['Released'] = words[1]\n",
    "        music['US Chart Position'] = words[4]\n",
    "        music['RIAA Certification'] = words[6]\n",
    "        music['BPI Certification'] = words[5]\n",
    "        data.append(music)\n",
    "    print(data[1:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='my_sol_parsing_csv'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Solution: Parsing CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "\n",
    "DATADIR = os.getcwd()\n",
    "DATAFILE = \"beatles-diskography.csv\"\n",
    "\n",
    "def parse_file(datafile):\n",
    "    data = []\n",
    "    with open(datafile, \"r\") as f:\n",
    "        for line in f:\n",
    "            music = dict()\n",
    "            line = line.strip()\n",
    "            words = line.split(',')\n",
    "            music['Title'] = words[0]\n",
    "            music['UK Chart Position'] = words[3]\n",
    "            music['Label'] = words[2]\n",
    "            music['Released'] = words[1]\n",
    "            music['US Chart Position'] = words[4]\n",
    "            music['RIAA Certification'] = words[6]\n",
    "            music['BPI Certification'] = words[5]\n",
    "            data.append(music)\n",
    "\n",
    "    print('My Solution')\n",
    "    pprint.pprint(data[1:11])\n",
    "    return data[1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='inst_sol_parsing_csv'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructor Solution: Parsing CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "\n",
    "DATADIR = os.getcwd()\n",
    "DATAFILE = \"beatles-diskography.csv\"\n",
    "\n",
    "def parse_file(datafile):\n",
    "    data = []\n",
    "    with open(datafile, \"r\") as f:\n",
    "        header = f.readline().split(',')\n",
    "        counter = 0\n",
    "        for line in f:\n",
    "            if counter == 10:\n",
    "                break\n",
    "                \n",
    "            fields = line.split(',')\n",
    "            entry = {}\n",
    "            \n",
    "            for i, value in enumerate(fields):\n",
    "                entry[header[i].strip()] = value.strip()\n",
    "                \n",
    "            data.append(entry)\n",
    "            counter += 1\n",
    "            \n",
    "    print('Instructor Solution')\n",
    "    pprint.pprint(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Parsing CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    # a simple test of your implemetation\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    d = parse_file(datafile)\n",
    "    firstline = {'Title': 'Please Please Me', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '22 March 1963', \n",
    "                 'US Chart Position': '-', 'RIAA Certification': 'Platinum', 'BPI Certification': 'Gold'}\n",
    "    tenthline = {'Title': '', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '10 July 1964', \n",
    "                 'US Chart Position': '-', 'RIAA Certification': '', 'BPI Certification': 'Gold'}\n",
    "\n",
    "    assert d[0] == firstline\n",
    "    assert d[9] == tenthline\n",
    "\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='csv_module'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With CSV Module\n",
    "\n",
    "https://docs.python.org/2/library/csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import csv\n",
    "\n",
    "DATADIR = os.getcwd()\n",
    "DATAFILE = \"beatles-diskography.csv\"\n",
    "\n",
    "def parse_csv(datafile):\n",
    "    data = []\n",
    "    n =  0\n",
    "    with open(datafile, 'r') as sd:\n",
    "        r = csv.DictReader(sd)\n",
    "        for line in r:\n",
    "            data.append(line)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = os.path.join(DATADIR, DATAFILE)\n",
    "parse_csv(datafile)\n",
    "d = parse_csv(datafile)\n",
    "pprint.pprint(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='intro_xlrd'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Intro to XLRD\n",
    "\n",
    "http://xlrd.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "\n",
    "datafile = '2013_ERCOT_Hourly_Load_Data.xls'\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    \n",
    "    data = [[sheet.cell_value(r, col)\n",
    "        for col in range(sheet.ncols)]\n",
    "            for r in range(sheet.nrows)]\n",
    "    \n",
    "    print(\"\\nList Comprehension\")\n",
    "    print(\"data[3][2]:\", data[3][2])\n",
    "\n",
    "    print(\"\\nCells in a nested loop:\")\n",
    "    for row in range(sheet.nrows):\n",
    "        for col in range(sheet.ncols):\n",
    "            if row == 50:\n",
    "                print(sheet.cell_value(row, col))\n",
    "\n",
    "\n",
    "    ### other useful methods:\n",
    "    print(\"\\nROWS, COLUMNS, and CELLS:\")\n",
    "    print(\"Number of rows in the sheet:\", sheet.nrows)\n",
    "    print(\"Type of data in cell (row 3, col 2):\", sheet.cell_type(3, 2))\n",
    "    print(\"Value in cell (row 3, col 2):\", sheet.cell_value(3, 2))\n",
    "    print(\"Get a slice of values in column 3, from rows 1-3:\", sheet.col_values(3, start_rowx=1, end_rowx=4))\n",
    "\n",
    "    print(\"\\nDATES:\")\n",
    "    print(\"Type of data in cell (row 1, col 0):\", sheet.cell_type(1, 0))\n",
    "    \n",
    "    exceltime = sheet.cell_value(1, 0)\n",
    "    \n",
    "    print(\"Time in Excel format:\", exceltime)\n",
    "    print(\"Convert time to a Python datetime tuple, from the Excel float:\", xlrd.xldate_as_tuple(exceltime, 0))\n",
    "\n",
    "    return data\n",
    "\n",
    "data = parse_file(datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='qz_reading_excel_files'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Reading Excel Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is as follows:\n",
    "- read the provided Excel file\n",
    "- find and return the min, max and average values for the COAST region\n",
    "- find and return the time value for the min and max entries\n",
    "- the time values should be returned as Python tuples\n",
    "\n",
    "Please see the test function for the expected return format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xlrd\n",
    "from zipfile import ZipFile\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "\n",
    "\n",
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='my_sol_reading_excel_files'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Solution: Reading Excel Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example on how you can get the data\n",
    "* sheet_data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)]\n",
    "\n",
    "#### other useful methods:\n",
    "* print(\"\\nROWS, COLUMNS, and CELLS:\")\n",
    "* print(\"Number of rows in the sheet:\", sheet.nrows)\n",
    "* print(\"Type of data in cell (row 3, col 2):\", sheet.cell_type(3, 2))\n",
    "* print(\"Value in cell (row 3, col 2):\", sheet.cell_value(3, 2))\n",
    "* print(\"Get a slice of values in column 3, from rows 1-3:\", sheet.col_values(3, start_rowx=1, end_rowx=4))\n",
    "* print(\"\\nDATES:\")\n",
    "* print(\"Type of data in cell (row 1, col 0):\", sheet.cell_type(1, 0))\n",
    "* exceltime = sheet.cell_value(1, 0)\n",
    "* print(\"Time in Excel format:\", exceltime)\n",
    "* print(\"Convert time to a Python datetime tuple, from the Excel float:\", xlrd.xldate_as_tuple(exceltime, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    \n",
    "    header = sheet.row_values(0, start_colx=0, end_colx=sheet.ncols)\n",
    "    coast_loc = header.index('COAST')\n",
    "    coast_col = sheet.col_values(coast_loc, start_rowx=1, end_rowx=sheet.nrows)\n",
    "    \n",
    "    maxtime_loc = coast_col.index(max(coast_col))\n",
    "    mintime_loc = coast_col.index(min(coast_col))\n",
    "    \n",
    "    # Use +1 because coast_col doesn't include the header (which is the first value)\n",
    "    maxtime_excel = sheet.cell_value(maxtime_loc+1, 0) \n",
    "    mintime_excel = sheet.cell_value(mintime_loc+1, 0)\n",
    "    \n",
    "    data = {\n",
    "            'maxtime': xlrd.xldate_as_tuple(maxtime_excel, 0),\n",
    "            'maxvalue': max(coast_col),\n",
    "            'mintime': xlrd.xldate_as_tuple(mintime_excel, 0),\n",
    "            'minvalue': min(coast_col),\n",
    "            'avgcoast': np.mean(coast_col)\n",
    "            }\n",
    "    \n",
    "    pprint.pprint(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='test_reading_excel_files'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Reading Excel Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    #open_zip(datafile)\n",
    "    data = parse_file(datafile)\n",
    "\n",
    "    assert data['maxtime'] == (2013, 8, 13, 17, 0, 0)\n",
    "    assert round(data['maxvalue'], 10) == round(18779.02551, 10)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='quiz_json'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: JSON\n",
    "\n",
    "* [JSON Tutorial](http://www.w3schools.com/js/js_json_intro.asp)\n",
    "* http://www.json.org\n",
    "* [Requests Documentation](http://requests.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "BASE_URL = \"http://musicbrainz.org/ws/2/\"\n",
    "ARTIST_URL = BASE_URL + \"artist/\"\n",
    "\n",
    "# query parameters are given to the requests.get function as a dictionary; this\n",
    "# variable contains some starter parameters.\n",
    "query_type = {  \"simple\": {},\n",
    "                \"atr\": {\"inc\": \"aliases+tags+ratings\"},\n",
    "                \"aliases\": {\"inc\": \"aliases\"},\n",
    "                \"releases\": {\"inc\": \"releases\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_site(url, params, uid=\"\", fmt=\"json\"):\n",
    "    # This is the main function for making queries to the musicbrainz API.\n",
    "    # A json document should be returned by the query.\n",
    "    params[\"fmt\"] = fmt\n",
    "    r = requests.get(url + uid, params=params)\n",
    "    print(\"requesting\", r.url)\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_by_name(url, params, name):\n",
    "    # This adds an artist name to the query parameters before making\n",
    "    # an API call to the function above.\n",
    "    params[\"query\"] = \"artist:\" + name\n",
    "    return query_site(url, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretty_print(data, indent=4):\n",
    "    # After we get our output, we can format it to be more readable\n",
    "    # by using this function.\n",
    "    if type(data) == dict:\n",
    "        print(json.dumps(data, indent=indent, sort_keys=True))\n",
    "    else:\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''\n",
    "    Modify the function calls and indexing below to answer the questions on\n",
    "    the next quiz. HINT: Note how the output we get from the site is a\n",
    "    multi-level JSON document, so try making print statements to step through\n",
    "    the structure one level at a time or copy the output to a separate output\n",
    "    file.\n",
    "    '''\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"Nirvana\")\n",
    "    pretty_print(results)\n",
    "\n",
    "    artist_id = results[\"artists\"][1][\"id\"]\n",
    "    print(\"\\nARTIST:\")\n",
    "    pretty_print(results[\"artists\"][1])\n",
    "\n",
    "    '''\n",
    "    artist_data = query_site(ARTIST_URL, query_type[\"releases\"], artist_id)\n",
    "    releases = artist_data[\"releases\"]\n",
    "    print(\"\\nONE RELEASE:\")\n",
    "    pretty_print(releases[0], indent=2)\n",
    "    release_titles = [r[\"title\"] for r in releases]\n",
    "\n",
    "    print(\"\\nALL TITLES:\")\n",
    "    for t in release_titles:\n",
    "        print(t)\n",
    "    '''\n",
    "    \n",
    "    print(\"\\nAlias:\")\n",
    "    pretty_print(results[\"artists\"][0]['aliases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Exploring JSON Data\n",
    "\n",
    "1. How many bands named 'First Aid Kit'? 2\n",
    "2. Begin_Area Name for Queen? London\n",
    "3. Spanish Alias for Beatles? Los Beatles\n",
    "4. Nirvana Disambiguation?  90s US grunge band\n",
    "5. When was One Direction formed?  2010-07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='quiz_csv_mod'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Quiz: Using the CSV Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can check the data in the dropdown in the top-left corner of the quiz starter code.\n",
    "\n",
    "Data comes from [NREL](www.nrel.gov) website. The datafile in this exercise is a small subset from the full file for one of the stations. You can download it from the Downloadables section > or see the full data files for other stations on the [National Solar Radiation Data Base](http://rredc.nrel.gov/solar/old_data/nsrdb/1991-2005/tmy3/by_USAFN.html).\n",
    "\n",
    "[Documentation on csv.reader on docs.python.org](http://docs.python.org/2/library/csv.html#csv.reader)\n",
    "\n",
    "[Documentation on Reader object methods on docs.python.org](http://docs.python.org/2/library/csv.html#reader-objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to process the supplied file and use the csv module to extract data from it.\n",
    "The data comes from NREL (National Renewable Energy Laboratory) website. Each file\n",
    "contains information from one meteorological station, in particular - about amount of\n",
    "solar and wind energy for each hour of day.\n",
    "\n",
    "Note that the first line of the datafile is neither data entry, nor header. It is a line\n",
    "describing the data source. You should extract the name of the station from it.\n",
    "\n",
    "The data should be returned as a list of lists (not dictionaries).\n",
    "You can use the csv modules \"reader\" method to get data in such format.\n",
    "Another useful method is next() - to get the next line from the iterator.\n",
    "You should only change the parse_file function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='my_sol_using_csv_mod'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "DATADIR = os.getcwd()\n",
    "DATAFILE = \"745090.csv\"\n",
    "\n",
    "datafile = os.path.join(DATADIR, DATAFILE)\n",
    "\n",
    "def parse_file(datafile):\n",
    "    name = \"\"\n",
    "    data = []\n",
    "    counter = 0\n",
    "    with open(datafile, 'r') as f:\n",
    "        test_reader = csv.reader(f)\n",
    "        for row in test_reader:\n",
    "            if counter == 0:\n",
    "                name = row[1]\n",
    "            if counter >= 2:\n",
    "                data.append(row)\n",
    "            \n",
    "            counter += 1\n",
    "\n",
    "    #print(name, data)\n",
    "    # Do not change the line below\n",
    "    return (name, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file(DATAFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    name, data = parse_file(datafile)\n",
    "\n",
    "    assert name == \"MOUNTAIN VIEW MOFFETT FLD NAS\"\n",
    "    assert data[0][1] == \"01:00\"\n",
    "    assert data[2][0] == \"01/01/2005\"\n",
    "    assert data[2][5] == \"2\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='quiz_excel_csv'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Excel to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Find the time and value of max load for each of the regions\n",
    "COAST, EAST, FAR_WEST, NORTH, NORTH_C, SOUTHERN, SOUTH_C, WEST\n",
    "and write the result out in a csv file, using pipe character | as the delimiter.\n",
    "\n",
    "An example output can be seen in the \"example.csv\" file.\n",
    "\n",
    "See csv module documentation on how to use different delimeters for csv.writer- http://docs.python.org/2/library/csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xlrd\n",
    "import os\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "import pprint\n",
    "\n",
    "DATADIR = os.getcwd()\n",
    "DATAFILE = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "datafile = os.path.join(DATADIR, DATAFILE)\n",
    "\n",
    "OUTFILE = \"2013_Max_Loads.csv\"\n",
    "outfile = os.path.join(DATADIR, OUTFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='my_sol_excel_csv'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution: Excel to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "    header = sheet.row_values(0, start_colx=0, end_colx=sheet.ncols)\n",
    "    \n",
    "    data = [('Station', 'Year', 'Month', 'Day', 'Hour', 'Max Load')]\n",
    "    \n",
    "    for station in header[1:-1]:\n",
    "        station_loc = header.index(station)\n",
    "        station_col = sheet.col_values(station_loc, start_rowx=1, end_rowx=sheet.nrows)\n",
    "    \n",
    "        maxtime_loc = station_col.index(max(station_col))\n",
    "        \n",
    "        # Use +1 because coast_col doesn't include the header (which is the first value)\n",
    "        maxtime_excel = sheet.cell_value(maxtime_loc+1, 0) \n",
    "        \n",
    "        xldate = xlrd.xldate_as_tuple(maxtime_excel, 0)[0:4]\n",
    "        \n",
    "        data.append((station, xldate[0], xldate[1], xldate[2], xldate[3], max(station_col)))\n",
    "        \n",
    "        \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_file(data, filename):\n",
    "    with open(filename, 'w', newline='') as myfile:\n",
    "        wr = csv.writer(myfile, delimiter = '|')\n",
    "        for row in data:\n",
    "            wr.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='inst_sol_excel_csv'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructor Solution: Excel to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    data = {}\n",
    "    # process all rows that contain station data\n",
    "    for n in range (1, 9):\n",
    "        station = sheet.cell_value(0, n)\n",
    "        cv = sheet.col_values(n, start_rowx=1, end_rowx=None)\n",
    "\n",
    "        maxval = max(cv)\n",
    "        maxpos = cv.index(maxval) + 1\n",
    "        maxtime = sheet.cell_value(maxpos, 0)\n",
    "        realtime = xlrd.xldate_as_tuple(maxtime, 0)\n",
    "        data[station] = {\"maxval\": maxval,\n",
    "                         \"maxtime\": realtime}\n",
    "\n",
    "    print data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_file(data, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        w = csv.writer(f, delimiter='|')\n",
    "        w.writerow([\"Station\", \"Year\", \"Month\", \"Day\", \"Hour\", \"Max Load\"])\n",
    "        for s in data:\n",
    "            year, month, day, hour, _ , _= data[s][\"maxtime\"]\n",
    "            w.writerow([s, year, month, day, hour, data[s][\"maxval\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='test_excel_csv'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Excel to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    #open_zip(datafile)\n",
    "    data = parse_file(datafile)\n",
    "    save_file(data, outfile)\n",
    "\n",
    "    number_of_rows = 0\n",
    "    stations = []\n",
    "\n",
    "    ans = {'FAR_WEST': {'Max Load': '2281.2722140000024',\n",
    "                        'Year': '2013',\n",
    "                        'Month': '6',\n",
    "                        'Day': '26',\n",
    "                        'Hour': '17'}}\n",
    "    correct_stations = ['COAST', 'EAST', 'FAR_WEST', 'NORTH',\n",
    "                        'NORTH_C', 'SOUTHERN', 'SOUTH_C', 'WEST']\n",
    "    fields = ['Year', 'Month', 'Day', 'Hour', 'Max Load']\n",
    "\n",
    "    with open(outfile) as of:\n",
    "        csvfile = csv.DictReader(of, delimiter=\"|\")\n",
    "        for line in csvfile:\n",
    "            station = line['Station']\n",
    "            if station == 'FAR_WEST':\n",
    "                for field in fields:\n",
    "                    # Check if 'Max Load' is within .1 of answer\n",
    "                    if field == 'Max Load':\n",
    "                        max_answer = round(float(ans[station][field]), 1)\n",
    "                        max_line = round(float(line[field]), 1)\n",
    "                        assert max_answer == max_line\n",
    "\n",
    "                    # Otherwise check for equality\n",
    "                    else:\n",
    "                        assert ans[station][field] == line[field]\n",
    "\n",
    "            number_of_rows += 1\n",
    "            stations.append(station)\n",
    "\n",
    "        # Output should be 8 lines not including header\n",
    "        assert number_of_rows == 8\n",
    "\n",
    "        # Check Station Names\n",
    "        assert set(stations) == set(correct_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='quiz_wrangling_json'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Wrangling JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise shows some important concepts that you should be aware about:\n",
    "- using codecs module to write unicode files\n",
    "- using authentication with web APIs\n",
    "- using offset when accessing web APIs\n",
    "\n",
    "To run this code locally you have to register at the NYTimes developer site \n",
    "and get your own API key. You will be able to complete this exercise in our UI\n",
    "without doing so, as we have provided a sample result. (See the file \n",
    "'popular-viewed-1.json' from the tabs above.)\n",
    "\n",
    "Your task is to modify the article_overview() function to process the saved\n",
    "file that represents the most popular articles (by view count) from the last\n",
    "day, and return a tuple of variables containing the following data:\n",
    "- labels: list of dictionaries, where the keys are the \"section\" values and\n",
    "  values are the \"title\" values for each of the retrieved articles.\n",
    "- urls: list of URLs for all 'media' entries with \"format\": \"Standard Thumbnail\"\n",
    "\n",
    "All your changes should be in the article_overview() function. See the test() \n",
    "function for examples of the elements of the output lists.\n",
    "The rest of functions are provided for your convenience, if you want to access\n",
    "the API by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the data in the dropdown in the top-left corner of the quiz starter code.\n",
    "\n",
    "If you want to know more, or query the site by yourself, please read the [NYTimes Developer Documentation for the Most Popular API](http://developer.nytimes.com/docs/most_popular_api/) and [apply for your own API Key for NY Times](http://developer.nytimes.com/page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import codecs\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "DATADIR = os.getcwd()\n",
    "DATAFILE = \"popular-{0}-{1}.json\"\n",
    "datafile = os.path.join(DATADIR, DATAFILE)\n",
    "\n",
    "URL_MAIN = \"http://api.nytimes.com/svc/\"\n",
    "URL_POPULAR = URL_MAIN + \"mostpopular/v2/\"\n",
    "API_KEY = { \"popular\": \"\",\n",
    "            \"article\": \"\"}\n",
    "\n",
    "def get_from_file(kind, period):\n",
    "    filename = datafile.format(kind, period)\n",
    "    with open(filename, \"r\") as f:\n",
    "        return json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='my_sol_wrangling_json'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution: Wrangling JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(get_from_file('viewed', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artist_id = results[\"artists\"][1][\"id\"]\n",
    "    print(\"\\nARTIST:\")\n",
    "    pretty_print(results[\"artists\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def article_overview(kind, period):\n",
    "    data = get_from_file(kind, period)\n",
    "\n",
    "    titles = []\n",
    "    urls =[]\n",
    "    \n",
    "    for d in data:\n",
    "        section = d['section']\n",
    "        title = d['title']\n",
    "        titles.append({section: title})\n",
    "        for m in d['media']:\n",
    "            for mm in m['media-metadata']:\n",
    "                if mm['format'] == 'Standard Thumbnail':\n",
    "                    urls.append(mm['url'])\n",
    " \n",
    "    return (titles, urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint(article_overview('viewed', 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='inst_sol_wrangling_json'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructor Solution: Wrangling JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def article_overview(kind, period):\n",
    "    data = get_from_file(kind, period)\n",
    "    titles = []\n",
    "    urls =[]\n",
    "\n",
    "    for article in data:\n",
    "        section = article[\"section\"]\n",
    "        title = article[\"title\"]\n",
    "        titles.append({section: title})\n",
    "        if \"media\" in article:\n",
    "            for m in article[\"media\"]:\n",
    "                for mm in m[\"media-metadata\"]:\n",
    "                    if mm[\"format\"] == \"Standard Thumbnail\":\n",
    "                        urls.append(mm[\"url\"])\n",
    "\n",
    "    return (titles, urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and Supplied Code: Wrangling JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_site(url, target, offset):\n",
    "    # This will set up the query with the API key and offset\n",
    "    # Web services often use offset paramter to return data in small chunks\n",
    "    # NYTimes returns 20 articles per request, if you want the next 20\n",
    "    # You have to provide the offset parameter\n",
    "    if API_KEY[\"popular\"] == \"\" or API_KEY[\"article\"] == \"\":\n",
    "        print(\"You need to register for NYTimes Developer account to run this program.\")\n",
    "        print (\"See Intructor notes for information\")\n",
    "        return False\n",
    "    params = {\"api-key\": API_KEY[target], \"offset\": offset}\n",
    "    r = requests.get(url, params = params)\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_popular(url, kind, days, section=\"all-sections\", offset=0):\n",
    "    # This function will construct the query according to the requirements of the site\n",
    "    # and return the data, or print an error message if called incorrectly\n",
    "    if days not in [1,7,30]:\n",
    "        print(\"Time period can be 1,7, 30 days only\")\n",
    "        return False\n",
    "    if kind not in [\"viewed\", \"shared\", \"emailed\"]:\n",
    "        print(\"kind can be only one of viewed/shared/emailed\")\n",
    "        return False\n",
    "\n",
    "    url += \"most{0}/{1}/{2}.json\".format(kind, section, days)\n",
    "    data = query_site(url, \"popular\", offset)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_file(kind, period):\n",
    "    # This will process all results, by calling the API repeatedly with supplied offset value,\n",
    "    # combine the data and then write all results in a file.\n",
    "    data = get_popular(URL_POPULAR, \"viewed\", 1)\n",
    "    num_results = data[\"num_results\"]\n",
    "    full_data = []\n",
    "    with codecs.open(\"popular-{0}-{1}.json\".format(kind, period), encoding='utf-8', mode='w') as v:\n",
    "        for offset in range(0, num_results, 20):        \n",
    "            data = get_popular(URL_POPULAR, kind, period, offset=offset)\n",
    "            full_data += data[\"results\"]\n",
    "        \n",
    "        v.write(json.dumps(full_data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    titles, urls = article_overview(\"viewed\", 1)\n",
    "    assert len(titles) == 20\n",
    "    assert len(urls) == 30\n",
    "    assert titles[2] == {'Opinion': 'Professors, We Need You!'}\n",
    "    assert urls[20] == 'http://graphics8.nytimes.com/images/2014/02/17/sports/ICEDANCE/ICEDANCE-thumbStandard.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='data_complex_formats'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data in More Complex Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## XML\n",
    "\n",
    "XML - Extensible Markup Language\n",
    "* https://www.w3schools.com/xml/xml_whatis.asp\n",
    "* https://www.w3.org/TR/xml/#sec-origin-goals\n",
    "* http://www.tizag.com/xmlTutorial/index.php\n",
    "* https://docs.python.org/2/library/xml.etree.elementtree.html#module-xml.etree.ElementTree\n",
    "* https://docs.python.org/3/library/xml.etree.elementtree.html#module-xml.etree.ElementTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pprint as pp\n",
    "\n",
    "tree = ET.parse('exampleresearcharticle.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "title = root.find('./fm/bibl/title')\n",
    "title_text = ''\n",
    "for p in title:\n",
    "    title_text += p.text\n",
    "print('\\nTitle:\\n', title_text)\n",
    "\n",
    "print('\\nAuthor email addresses:')\n",
    "for a in root.findall('./fm/bibl/aug/au'):\n",
    "    email = a.find('email')\n",
    "    if email is not None:\n",
    "        print(email.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='quiz_extracting_data_xml'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Extracting Data XML\n",
    "\n",
    "1. Your task here is to extract data from xml on authors of an article and add it to a list, one item for an author.\n",
    "2. See the provided data structure for the expected format.\n",
    "3. The tags for first name, surname and email should map directly to the dictionary keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "article_file = \"exampleResearchArticle.xml\"\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_authors(root):\n",
    "    authors = []\n",
    "    for author in root.findall('./fm/bibl/aug/au'):\n",
    "        data = {\n",
    "                \"fnm\": None,\n",
    "                \"snm\": None,\n",
    "                \"email\": None\n",
    "        }\n",
    "        # MY CODE HERE\n",
    "\n",
    "        fnm = author.find('fnm')\n",
    "        snm = author.find('snm')\n",
    "        email = author.find('email')\n",
    "        \n",
    "        authors.append({'fnm': fnm.text, 'snm': snm.text, 'email': email.text})\n",
    "\n",
    "    return authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_authors(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructor Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_author(root):\n",
    "    authors = []\n",
    "    for author in root.findall('./fm/bibl/aug/au'):\n",
    "        data = {\n",
    "                \"fnm\": None,\n",
    "                \"snm\": None,\n",
    "                \"email\": None\n",
    "        }\n",
    "        data[\"fnm\"] = author.find('./fnm').text\n",
    "        data[\"snm\"] = author.find('./snm').text\n",
    "        data[\"email\"] = author.find('./email').text\n",
    "\n",
    "        authors.append(data)\n",
    "\n",
    "    return authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    solution = [{'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'},\n",
    "                {'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'},\n",
    "                {'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'},\n",
    "                {'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'},\n",
    "                {'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'},\n",
    "                {'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'},\n",
    "                {'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'},\n",
    "                {'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}]\n",
    "    \n",
    "    root = get_root(article_file)\n",
    "    data = get_authors(root)\n",
    "\n",
    "    assert data[0] == solution[0]\n",
    "    assert data[1][\"fnm\"] == solution[1][\"fnm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='quiz_handling_attributes_xml'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Handling Attributes XML\n",
    "\n",
    "1. Your task here is to extract data from xml on authors of an article and add it to a list, one item for an author.\n",
    "2. See the provided data structure for the expected format.\n",
    "3. The tags for first name, surname and email should map directly to the dictionary keys, but you have to extract the attributes from the \"insr\" tag and add them to the list for the dictionary key \"insr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "article_file = \"exampleResearchArticle.xml\"\n",
    "\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_authors(root):\n",
    "    authors = []\n",
    "    for author in root.findall('./fm/bibl/aug/au'):\n",
    "        data = {\n",
    "                \"fnm\": None,\n",
    "                \"snm\": None,\n",
    "                \"email\": None,\n",
    "                \"insr\": []\n",
    "        }\n",
    "\n",
    "        data['fnm'] = author.find('./fnm').text\n",
    "        data['snm'] = author.find('./snm').text\n",
    "        data['email'] = author.find('./email').text\n",
    "        \n",
    "        for insr in author.iter('insr'):\n",
    "            #print(insr.attrib)\n",
    "            data['insr'].append(insr.attrib['iid'])\n",
    "\n",
    "        authors.append(data)\n",
    "\n",
    "    return authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_authors(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructor Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_authors(root):\n",
    "    authors = []\n",
    "    for author in root.findall('./fm/bibl/aug/au'):\n",
    "        data = {\n",
    "                \"fnm\": None,\n",
    "                \"snm\": None,\n",
    "                \"email\": None,\n",
    "                \"insr\": []\n",
    "        }\n",
    "        data[\"fnm\"] = author.find('./fnm').text\n",
    "        data[\"snm\"] = author.find('./snm').text\n",
    "        data[\"email\"] = author.find('./email').text\n",
    "        insr = author.findall('./insr')\n",
    "        for i in insr:\n",
    "            data[\"insr\"].append(i.attrib[\"iid\"])\n",
    "        authors.append(data)\n",
    "\n",
    "    return authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    solution = [{'insr': ['I1'], 'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'},\n",
    "                {'insr': ['I2'], 'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'},\n",
    "                {'insr': ['I3', 'I4'], 'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'},\n",
    "                {'insr': ['I3'], 'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'},\n",
    "                {'insr': ['I8'], 'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'},\n",
    "                {'insr': ['I3', 'I5'], 'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'},\n",
    "                {'insr': ['I6'], 'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'},\n",
    "                {'insr': ['I7'], 'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}]\n",
    "\n",
    "    root = get_root(article_file)\n",
    "    data = get_authors(root)\n",
    "\n",
    "    assert data[0] == solution[0]\n",
    "    assert data[1][\"insr\"] == solution[1][\"insr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='quiz_beautiful_soup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Using Beautiful Soup\n",
    "\n",
    "1. Please note that the function 'make_request' is provided for your reference only.\n",
    "2. You will not be able to to actually use it from within the Udacity web UI.\n",
    "3. Your task is to process the HTML using [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), extract the hidden form field values for \"_EVENTVALIDATION\" and \"_VIEWSTATE\" and set the appropriate values in the data dictionary.\n",
    "\n",
    "4. All your changes should be in the 'extract_data' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "html_page = \"page_source.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(open(html_page), 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_class_but_no_id(tag):\n",
    "    return tag.has_attr('type') and tag.has_attr('name') and tag.has_attr('id') and tag.has_attr('value')\n",
    "\n",
    "soup.find_all(has_class_but_no_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = ['__EVENTVALIDATION', '__VIEWSTATE']\n",
    "for item in list:\n",
    "    my_tag = (soup.find(id=item))\n",
    "    print(my_tag['value'])\n",
    "    print('\\n''\\n''\\n''\\n''\\n''\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(page):\n",
    "    data = {\"eventvalidation\": \"\",\n",
    "            \"viewstate\": \"\"}\n",
    "    \n",
    "    soup = BeautifulSoup(open(page), 'html.parser')\n",
    "    \n",
    "    list = ['__EVENTVALIDATION', '__VIEWSTATE']\n",
    "    for item in list:\n",
    "        my_tag = (soup.find(id=item))\n",
    "        if item == '__EVENTVALIDATION':\n",
    "            data['eventvalidation'] = my_tag['value']\n",
    "        if item == '__VIEWSTATE':\n",
    "            data['viewstate'] = my_tag['value']\n",
    "    \n",
    "    return data\n",
    "\n",
    "extract_data(html_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructor Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_data(page):\n",
    "    data = {\"eventvalidation\": \"\",\n",
    "            \"viewstate\": \"\"}\n",
    "    with open(page, \"r\") as html:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        ev = soup.find(id=\"__EVENTVALIDATION\")\n",
    "        data[\"eventvalidation\"] = ev[\"value\"]\n",
    "\n",
    "        vs = soup.find(id=\"__VIEWSTATE\")\n",
    "        data[\"viewstate\"] = vs[\"value\"]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "\n",
    "    r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                    data={'AirportList': \"BOS\",\n",
    "                          'CarrierList': \"VX\",\n",
    "                          'Submit': 'Submit',\n",
    "                          \"__EVENTTARGET\": \"\",\n",
    "                          \"__EVENTARGUMENT\": \"\",\n",
    "                          \"__EVENTVALIDATION\": eventvalidation,\n",
    "                          \"__VIEWSTATE\": viewstate\n",
    "                    })\n",
    "\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    data = extract_data(html_page)\n",
    "    assert data[\"eventvalidation\"] != \"\"\n",
    "    assert data[\"eventvalidation\"].startswith(\"/wEWjAkCoIj1ng0\")\n",
    "    assert data[\"viewstate\"].startswith(\"/wEPDwUKLTI\")\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='quiz_carrier_list'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Quiz: Carrier List\n",
    "\n",
    "1. Your task in this exercise is to modify 'extract_carrier()' to get a list of all airlines. \n",
    "2. Exclude all of the combination values like 'All U.S. Carriers' from the data that you return.\n",
    "3. You should return a list of codes for the carriers.\n",
    "4. All your changes should be in the 'extract_carrier()' function. \n",
    "5. The 'options.html' file in the tab above is a stripped down version of what is actually on the website, but should provide an example of what you should get from the full file.\n",
    "6. Please note that the function 'make_request()' is provided for your reference only. You will not be able to to actually use it from within the Udacity web UI.\n",
    "\n",
    "All data is from:\n",
    "[TranStats website](https://www.transtats.bts.gov/Data_Elements.aspx?Data=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html_page = \"options.html\"\n",
    "\n",
    "\n",
    "def extract_carriers(page):\n",
    "    data = []\n",
    "\n",
    "    with open(page, \"r\") as html:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        cl = soup.find(id='CarrierList')\n",
    "\n",
    "        tag = cl.find_all('option')\n",
    "        for value in tag:\n",
    "\n",
    "            if value['value'] != 'All' and value['value'] != 'AllUS' and value['value'] != 'AllForeign' :\n",
    "                data.append(value['value'])\n",
    "        \n",
    "        \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_carriers(html_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "    airport = data[\"airport\"]\n",
    "    carrier = data[\"carrier\"]\n",
    "\n",
    "    r = s.post(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "               data = ((\"__EVENTTARGET\", \"\"),\n",
    "                       (\"__EVENTARGUMENT\", \"\"),\n",
    "                       (\"__VIEWSTATE\", viewstate),\n",
    "                       (\"__VIEWSTATEGENERATOR\",viewstategenerator),\n",
    "                       (\"__EVENTVALIDATION\", eventvalidation),\n",
    "                       (\"CarrierList\", carrier),\n",
    "                       (\"AirportList\", airport),\n",
    "                       (\"Submit\", \"Submit\")))\n",
    "\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    data = extract_carriers(html_page)\n",
    "    assert len(data) == 16\n",
    "    assert \"FL\" in data\n",
    "    assert \"NK\" in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='quiz_airport_list'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Airport List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution\n",
    "\n",
    "1. Complete the 'extract_airports()' function so that it returns a list of airport codes, excluding any combinations like \"All\".\n",
    "2. Refer to the 'options.html' file in the tab above for a stripped down version of what is actually on the website. The test() assertions are based on the given file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html_page = \"options.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_airports(page):\n",
    "    data = []\n",
    "    with open(page, \"r\") as html:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        \n",
    "        al = soup.find(id='AirportList')\n",
    "\n",
    "        tag = al.find_all('option')\n",
    "        for value in tag:\n",
    "\n",
    "            if value['value'] != 'All' and value['value'] != 'AllMajors' and value['value'] != 'AllOthers':\n",
    "                data.append(value['value'])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_airports(html_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    data = extract_airports(html_page)\n",
    "    assert len(data) == 15\n",
    "    assert \"ATL\" in data\n",
    "    assert \"ABR\" in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='quiz_processing_all'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Processing All\n",
    "\n",
    "Let's assume that you combined the code from the previous 2 exercises with code\n",
    "from the lesson on how to build requests, and downloaded all the data locally.\n",
    "The files are in a directory \"data\", named after the carrier and airport:\n",
    "\"{}-{}.html\".format(carrier, airport), for example \"FL-ATL.html\".\n",
    "\n",
    "The table with flight info has a table class=\"dataTDRight\". Your task is to\n",
    "use 'process_file()' to extract the flight data from that table as a list of\n",
    "dictionaries, each dictionary containing relevant data from the file and table\n",
    "row. This is an example of the data structure you should return:\n",
    "\n",
    "```data = [{\"courier\": \"FL\",\n",
    "         \"airport\": \"ATL\",\n",
    "         \"year\": 2012,\n",
    "         \"month\": 12,\n",
    "         \"flights\": {\"domestic\": 100,\n",
    "                     \"international\": 100}\n",
    "        },\n",
    "         {\"courier\": \"...\"}\n",
    "]\n",
    "```\n",
    "\n",
    "Note - year, month, and the flight data should be integers.\n",
    "You should skip the rows that contain the TOTAL data for a year.\n",
    "\n",
    "There are a couple of helper functions to deal with the data files.\n",
    "Please do not change them for grading purposes.\n",
    "All your changes should be in the 'process_file()' function.\n",
    "\n",
    "The 'data/FL-ATL.html' file in the tab above is only a part of the full data,\n",
    "covering data through 2003. The test() code will be run on the full table, but\n",
    "the given file should provide an example of what you will get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "html_page = \"AA-ATL.html\"\n",
    "#html_page = \"FL-ATL.html\"\n",
    "\n",
    "datadir = \"data\"\n",
    "\n",
    "\n",
    "def open_zip(datadir):\n",
    "    with ZipFile('{0}.zip'.format(datadir), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def process_all(datadir):\n",
    "    files = os.listdir(datadir)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_file(f):\n",
    "    '''\n",
    "    This function extracts data from the file given as the function argument in\n",
    "    a list of dictionaries. This is example of the data structure you should\n",
    "    return:\n",
    "\n",
    "    data = [{\"courier\": \"FL\",\n",
    "             \"airport\": \"ATL\",\n",
    "             \"year\": 2012,\n",
    "             \"month\": 12,\n",
    "             \"flights\": {\"domestic\": 100,\n",
    "                         \"international\": 100}\n",
    "            },\n",
    "            {\"courier\": \"...\"}\n",
    "    ]\n",
    "\n",
    "    Note - year, month, and the flight data should be integers.\n",
    "    You should skip the rows that contain the TOTAL data for a year.\n",
    "    '''\n",
    "    data = []\n",
    "    info = {}\n",
    "    #info[\"courier\"], info[\"airport\"] = f[:6].split(\"-\")\n",
    "    # Note: create a new dictionary for each entry in the output data list.\n",
    "    # If you use the info dictionary defined here each element in the list \n",
    "    # will be a reference to the same info dictionary.\n",
    "    with open(\"{}/{}\".format(datadir, f), \"r\") as html:\n",
    "\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "        # Extract the carrier and airport from within the file\n",
    "        def has_value_selected(tag):\n",
    "            return tag.has_attr('value') and tag.has_attr('selected')\n",
    "        \n",
    "        cl = soup.find(id='CarrierList')\n",
    "        cl_value = cl.find(has_value_selected)['value']\n",
    "        \n",
    "        al = soup.find(id='AirportList')\n",
    "        al_value = al.find(has_value_selected)['value']\n",
    "        \n",
    "        # Ignore values = All, AllMajors and AllOthers\n",
    "        if al_value != 'All' and al_value != 'AllMajors' and al_value != 'AllOthers':\n",
    "\n",
    "            # Parse the flight information table\n",
    "            for row in soup.select('table.dataTDRight tr')[1:]:\n",
    "                aux = row.findAll('td')\n",
    "                if aux[1].string != 'TOTAL':\n",
    "\n",
    "                    aux_2 = int((aux[2].string).replace(',', ''))\n",
    "                    aux_3 = int(((aux[3].string).replace(u'\\xa0', u'0')).replace(',', ''))\n",
    "\n",
    "                    data.append({'courier': cl_value,\n",
    "                                 'airport': al_value,\n",
    "                                 'year': int(aux[0].string), 'month': int(aux[1].string),\n",
    "                                 'flights': {'domestic': aux_2,\n",
    "                                             'international': aux_3}\n",
    "                                 })\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_file(html_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_file(f):\n",
    "\n",
    "    data = []\n",
    "    info = {}\n",
    "    \n",
    "    # Use info dict to supply the courier and airport code\n",
    "    info[\"courier\"], info[\"airport\"] = f[:6].split(\"-\")\n",
    "\n",
    "    with open(\"{}/{}\".format(datadir, f), \"r\") as html:\n",
    "\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "        # Parse the flight data table\n",
    "        for row in soup.select('table.dataTDRight tr')[1:]:\n",
    "            aux = row.findAll('td')\n",
    "            if aux[1].string != 'TOTAL':\n",
    "\n",
    "                aux_2 = int((aux[2].string).replace(',', ''))\n",
    "                aux_3 = int(((aux[3].string).replace(u'\\xa0', u'0')).replace(',', ''))\n",
    "\n",
    "                info.update({'year': int(aux[0].string), 'month': int(aux[1].string),\n",
    "                             'flights': {'domestic': aux_2, 'international': aux_3}\n",
    "                             })\n",
    "                \n",
    "                data.append(info)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_file(html_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructor Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_file(f):\n",
    "\n",
    "    data = []\n",
    "    info = {}\n",
    "    info[\"courier\"], info[\"airport\"] = f[:6].split(\"-\")\n",
    "\n",
    "    with open(\"{}/{}\".format(datadir, f), \"r\") as html:\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "        table_row = soup.find_all(\"tr\", class_ = 'dataTDRight')\n",
    "        table_data = []\n",
    "        for row in table_row:\n",
    "            table_col = []\n",
    "            for col in row.find_all(\"td\"):\n",
    "                table_col.append(col.get_text())\n",
    "            table_data.append(table_col)    \n",
    "        \n",
    "        for cell in table_data:\n",
    "            if cell[1] == \"TOTAL\":\n",
    "                continue\n",
    "            else: \n",
    "                \n",
    "                infoN ={}\n",
    "                flight = {}\n",
    "        \n",
    "                infoN['year'] = int(cell[0])\n",
    "                infoN['month'] = int(cell[1])\n",
    "                flight['domestic'] = int(cell[2].replace(\",\",\"\"))\n",
    "                flight['international'] = int(cell[3].replace(\",\",\"\")) \n",
    "                infoN['flights'] = flight \n",
    "                \n",
    "                info.update(infoN)\n",
    "                data.append(info) \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    print('Running a simple test...')\n",
    "    #open_zip(datadir)\n",
    "    data = process_file(html_page)\n",
    "\n",
    "    assert len(data) == 170  # Total number of rows\n",
    "    for entry in data[:3]:\n",
    "        assert type(entry[\"year\"]) == int\n",
    "        assert type(entry[\"month\"]) == int\n",
    "        assert type(entry[\"flights\"][\"domestic\"]) == int\n",
    "        assert len(entry[\"airport\"]) == 3\n",
    "        assert len(entry[\"courier\"]) == 2\n",
    "    assert data[0][\"courier\"] == 'AA'\n",
    "    assert data[0][\"month\"] == 10\n",
    "    assert data[-1][\"airport\"] == \"ATL\"\n",
    "    assert data[-1][\"flights\"] == {'international': 0, 'domestic': 1077}\n",
    "    \n",
    "    print('... success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='quiz_patent_database'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Patent Database\n",
    "\n",
    "This and the following exercise are using the US Patent database. The patent.data\n",
    "file is a small excerpt of much larger datafiles that are available for\n",
    "download from US Patent website. These files are pretty large ( >100 MB each).\n",
    "The original file is ~600MB large, you might not be able to open it in a text\n",
    "editor.\n",
    "\n",
    "The data itself is in XML, however there is a problem with how it's formatted.\n",
    "Please run this script and observe the error. Then find the line that is\n",
    "causing the error. You can do that by just looking at the datafile in the web\n",
    "UI, or programmatically. For quiz purposes it does not matter, but as an\n",
    "exercise we suggest that you try to do it programmatically.\n",
    "\n",
    "NOTE: You do not need to correct the error - for now, just find where the error\n",
    "is occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "PATENTS = 'patent.data'\n",
    "\n",
    "def get_root(fname):\n",
    "\n",
    "    tree = ET.parse(fname)\n",
    "    print(tree)\n",
    "    return tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_root(PATENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='quiz_result_parsing_datafile'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Result of Processing the Datafile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There are multiple xml trees in the same file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='quiz_processing_patents'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Processing Patents\n",
    "\n",
    "1. The problem is that the gigantic file is actually not a valid XML\n",
    "    * It has several root elements\n",
    "    * Is also has XML declarations\n",
    "2. It's a collection of a lot of concatenated XML documents.\n",
    "3. One solution is to split the file into separate documents, so you can process the resulting files as valid XML documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "PATENTS = 'patent.data'\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_file(filename):\n",
    "    \"\"\"\n",
    "    Split the input file into separate files, each containing a single patent.\n",
    "    As a hint - each patent declaration starts with the same line that was\n",
    "    causing the error found in the previous exercises.\n",
    "    \n",
    "    The new files should be saved with filename in the following format:\n",
    "    \"{}-{}\".format(filename, n) where n is a counter, starting from 0.\n",
    "    \"\"\"\n",
    "    infile = open(PATENTS, 'r')\n",
    "    expected_first_line = '<?xml version=\"1.0\" encoding=\"UTF-8\"?>'\n",
    "\n",
    "    n = 0\n",
    "    for line in infile.readlines():\n",
    "\n",
    "\n",
    "        if line.strip() == expected_first_line.strip():\n",
    "            outfile = open('{}-{}'.format(filename, n), 'w')\n",
    "            n+=1\n",
    "\n",
    "        outfile.write(line.strip())\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_file(PATENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    split_file(PATENTS)\n",
    "    for n in range(4):\n",
    "        try:\n",
    "            fname = \"{}-{}\".format(PATENTS, n)\n",
    "            f = open(fname, \"r\")\n",
    "            if not f.readline().startswith(\"<?xml\"):\n",
    "                print('You have not split the file {} in the correct boundary!').format(fname)\n",
    "            f.close()\n",
    "        except:\n",
    "            print('Could not find file {}. Check if the filename is correct!').format(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='data_quality'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='quiz_correcting_validity'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Correcting Validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to check the \"productionStartYear\" of the DBPedia autos datafile for valid values.\n",
    "The following things should be done:\n",
    "1. Check if the field \"productionStartYear\" contains a year\n",
    "2. Check if the year is in range 1886-2014\n",
    "3. Convert the value of the field to be just a year (not full datetime)\n",
    "4. The rest of the fields and values should stay the same\n",
    "5. If the value of the field is a valid year in the range as described above, write that line to the output_good file\n",
    "6. If the value of the field is not a valid year as described above,  write that line to the output_bad file\n",
    "7. Discard rows (neither write to good nor bad) if the URI is not from dbpedia.org\n",
    "8. You should use the provided way of reading and writing data (DictReader and DictWriter), they will take care of dealing with the header.\n",
    "\n",
    "You can write helper functions for checking the data and writing the files, but we will call only the \n",
    "'process_file' with 3 arguments (inputfile, output_good, output_bad)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pprint\n",
    "\n",
    "INPUT_FILE = 'autos.csv'\n",
    "OUTPUT_GOOD = 'autos-valid.csv'\n",
    "OUTPUT_BAD = 'FIXME-autos.csv'\n",
    "\n",
    "def process_file(input_file, output_good, output_bad):\n",
    "    \n",
    "    with open(input_file, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        header = reader.fieldnames\n",
    "        \n",
    "        auto_good = []\n",
    "        auto_bad = []\n",
    "        \n",
    "        uri_match = 'http://dbpedia.org/resource'\n",
    "    \n",
    "        for row in reader:\n",
    "                        \n",
    "            if row['URI'][0:27] == uri_match:\n",
    "                \n",
    "                row['productionStartYear'] = row['productionStartYear'][0:4].strip()\n",
    "                prod_year = row['productionStartYear']\n",
    "                \n",
    "                if prod_year == 'NULL':\n",
    "                    print(prod_year, ' Bad')\n",
    "                    auto_bad.append(row)\n",
    "                    continue                    \n",
    "                if int(prod_year) >= 1886 and int(prod_year) <= 2014:\n",
    "                    print(prod_year, ' Good')\n",
    "                    auto_good.append(row)\n",
    "                    continue                    \n",
    "                else:\n",
    "                    print(prod_year, ' Bad')                    \n",
    "                    auto_bad.append(row)\n",
    "\n",
    "    # This is just an example on how you can use csv.DictWriter\n",
    "    # Remember that you have to output 2 files\n",
    "    with open(output_good, \"w\", newline='') as g: # newline for python3 (no extra lines)\n",
    "        writer = csv.DictWriter(g, fieldnames= header)\n",
    "        writer.writeheader()\n",
    "        for row in auto_good:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "    with open(output_bad, \"w\") as b:\n",
    "        writer = csv.DictWriter(b, delimiter=\",\", fieldnames= header) # delimiter for python2\n",
    "        writer.writeheader()\n",
    "        for row in auto_bad:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_file(INPUT_FILE, OUTPUT_GOOD, OUTPUT_BAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructor Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_file(input_file, output_good, output_bad):\n",
    "    # store data into lists for output\n",
    "    data_good = []\n",
    "    data_bad = []\n",
    "    with open(input_file, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        header = reader.fieldnames\n",
    "        for row in reader:\n",
    "            # validate URI value\n",
    "            if row['URI'].find(\"dbpedia.org\") < 0:\n",
    "                continue\n",
    "\n",
    "            ps_year = row['productionStartYear'][:4]\n",
    "            try: # use try/except to filter valid items\n",
    "                ps_year = int(ps_year)\n",
    "                row['productionStartYear'] = ps_year\n",
    "                if (ps_year >= 1886) and (ps_year <= 2014):\n",
    "                    data_good.append(row)\n",
    "                else:\n",
    "                    data_bad.append(row)\n",
    "            except ValueError: # non-numeric strings caught by exception\n",
    "                if ps_year == 'NULL':\n",
    "                    data_bad.append(row)\n",
    "\n",
    "    # Write processed data to output files\n",
    "    with open(output_good, \"w\") as good:\n",
    "        writer = csv.DictWriter(good, delimiter=\",\", fieldnames= header)\n",
    "        writer.writeheader()\n",
    "        for row in data_good:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    with open(output_bad, \"w\") as bad:\n",
    "        writer = csv.DictWriter(bad, delimiter=\",\", fieldnames= header)\n",
    "        writer.writeheader()\n",
    "        for row in data_bad:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "\n",
    "    process_file(INPUT_FILE, OUTPUT_GOOD, OUTPUT_BAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='quiz_auditing_data_quality'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Auditing Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem set, you work with cities infobox data, audit it, come up with a\n",
    "cleaning idea and then clean it up. In the first exercise we want you to audit\n",
    "the datatypes that can be found in some particular fields in the dataset.\n",
    "The possible types of values can be:\n",
    "\n",
    "- NoneType if the value is a string \"NULL\" or an empty string \"\"\n",
    "- list, if the value starts with \"{\"\n",
    "- int, if the value can be cast to int\n",
    "- float, if the value can be cast to float, but CANNOT be cast to int.\n",
    "   For example, '3.23e+07' should be considered a float because it can be cast\n",
    "   as float but int('3.23e+07') will throw a ValueError\n",
    "- 'str', for all other values\n",
    "\n",
    "The audit_file function should return a dictionary containing fieldnames and a \n",
    "SET of the types that can be found in the field. e.g.\n",
    "{\"field1\": set([type(float()), type(int()), type(str())]),\n",
    " \"field2\": set([type(str())]),\n",
    "  ....\n",
    "}\n",
    "The type() function returns a type object describing the argument given to the \n",
    "function. You can also use examples of objects to create type objects, e.g.\n",
    "type(1.1) for a float: see the test function below for examples.\n",
    "\n",
    "Note that the first three rows (after the header row) in the cities.csv file\n",
    "are not actual data points. The contents of these rows should note be included\n",
    "when processing data types. Be sure to include functionality in your code to\n",
    "skip over or detect these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "cities = 'cities.csv'\n",
    "cities_full = 'cities_full.csv'\n",
    "\n",
    "FIELDS = [\"name\", \"timeZone_label\", \"utcOffset\", \"homepage\", \"governmentType_label\",\n",
    "          \"isPartOf_label\", \"areaCode\", \"populationTotal\", \"elevation\",\n",
    "          \"maximumElevation\", \"minimumElevation\", \"populationDensity\",\n",
    "          \"wgs84_pos#lat\", \"wgs84_pos#long\", \"areaLand\", \"areaMetro\", \"areaUrban\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_list(s):\n",
    "    if s[0] == '{':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_int(s):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_float(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audit_file(filename, fields):\n",
    "    fieldtypes = {}\n",
    "    \n",
    "    for field in fields:\n",
    "        fieldtypes[f'{field}'] = set()\n",
    "\n",
    "    with open(filename, 'r', encoding='utf8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        header = reader.fieldnames\n",
    "        for row in reader:\n",
    "            # validate URI value\n",
    "            if row['URI'].find(\"dbpedia.org\") < 0:\n",
    "                continue\n",
    "\n",
    "            for field in fields:\n",
    "\n",
    "                if row[field] == ('NULL' or ''):\n",
    "                    fieldtypes[field].add(type(None))\n",
    "                elif is_list(row[field]) is True:\n",
    "                    fieldtypes[field].add(type([]))\n",
    "                elif is_int(row[field]) is True:\n",
    "                    fieldtypes[field].add(type(1))\n",
    "                elif is_float(row[field]) is True:\n",
    "                    fieldtypes[field].add(type(1.1))\n",
    "                else:\n",
    "                    fieldtypes[field].add(type(''))\n",
    "\n",
    "\n",
    "    return fieldtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    fieldtypes = audit_file(cities, FIELDS)\n",
    "\n",
    "    pprint.pprint(fieldtypes)\n",
    "\n",
    "    assert fieldtypes[\"areaLand\"] == set([type(1.1), type([]), type(None)])\n",
    "    assert fieldtypes['areaMetro'] == set([type(1.1), type(None)])\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='quiz_fixing_area'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Fixing the Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem set you work with cities infobox data, audit it, come up with a\n",
    "cleaning idea and then clean it up.\n",
    "\n",
    "Since in the previous quiz you made a decision on which value to keep for the\n",
    "\"areaLand\" field, you now know what has to be done.\n",
    "\n",
    "Finish the function fix_area(). It will receive a string as an input, and it\n",
    "has to return a float representing the value of the area or None.\n",
    "You have to change the function fix_area. You can use extra functions if you\n",
    "like, but changes to process_file will not be taken into account.\n",
    "The rest of the code is just an example on how this function can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "CITIES = 'cities.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_area(areas):\n",
    "\n",
    "    def is_list(s):\n",
    "        if s[0] == '{':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def is_float(s):\n",
    "        try:\n",
    "            float(s)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    \n",
    "    values = []\n",
    "    \n",
    "    clean_areas =  str.maketrans('', '', '}{')\n",
    "    \n",
    "    if areas == ('NULL' or ''):\n",
    "        return None\n",
    "        \n",
    "    elif is_float(areas) is True:\n",
    "        return float(areas)\n",
    "        \n",
    "    elif is_list(areas) is True:\n",
    "        areas = [s.translate(clean_areas) for s in areas]\n",
    "        areas = (''.join(areas)).split('|')\n",
    "                \n",
    "        for area in areas:\n",
    "            area_len = len(((area.lower()).split('e')[0]).rstrip('0'))\n",
    "            values.append(area_len)\n",
    "            \n",
    "        if values[0] >= values[1]:\n",
    "            return float(areas[0])\n",
    "        else:\n",
    "            return float(areas[1])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(filename):\n",
    "    # CHANGES TO THIS FUNCTION WILL BE IGNORED WHEN YOU SUBMIT THE EXERCISE\n",
    "    data = []\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "\n",
    "        #skipping the extra metadata\n",
    "        for i in range(3):\n",
    "            l = next(reader)\n",
    "\n",
    "        # processing file\n",
    "        for line in reader:\n",
    "            # calling your function to fix the area value\n",
    "            if \"areaLand\" in line:\n",
    "                print('Old: ', line['areaLand'])\n",
    "                line[\"areaLand\"] = fix_area(line[\"areaLand\"])\n",
    "                print('New: ', line['areaLand'], '\\n')\n",
    "            data.append(line)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    data = process_file(CITIES)\n",
    "\n",
    "    print('Printing three example results:')\n",
    "    for n in range(5,8):\n",
    "        pprint.pprint(data[n][\"areaLand\"])\n",
    "\n",
    "    assert data[3][\"areaLand\"] == None        \n",
    "    assert data[8][\"areaLand\"] == 55166700.0\n",
    "    assert data[20][\"areaLand\"] == 14581600.0\n",
    "    assert data[33][\"areaLand\"] == 20564500.0\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='quiz_fixing_name'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Fixing Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem set you work with cities infobox data, audit it, come up with a\n",
    "cleaning idea and then clean it up.\n",
    "\n",
    "In the previous quiz you recognized that the \"name\" value can be an array (or\n",
    "list in Python terms). It would make it easier to process and query the data\n",
    "later if all values for the name are in a Python list, instead of being\n",
    "just a string separated with special characters, like now.\n",
    "\n",
    "Finish the function fix_name(). It will recieve a string as an input, and it\n",
    "will return a list of all the names. If there is only one name, the list will\n",
    "have only one item in it; if the name is \"NULL\", the list should be empty.\n",
    "The rest of the code is just an example on how this function can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import csv\n",
    "import pprint\n",
    "\n",
    "CITIES = 'cities.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from string import maketrans  # use if python 2.7\n",
    "\n",
    "def clean_string(dirty_string):\n",
    "    string_strip = str.maketrans('', '', '}{')\n",
    "    #string_strip =  maketrans('}{', '  ')  # use if python 2.7\n",
    "    \n",
    "    dirty_string = [s.translate(string_strip) for s in dirty_string]\n",
    "    dirty_string = ((''.join(dirty_string)).strip()).split('|')\n",
    "    \n",
    "    return dirty_string\n",
    "\n",
    "\n",
    "def is_list(s):\n",
    "    if s[0] == '{':\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '{Negtemiut|Nightmute}'\n",
    "if is_list(test) is True:\n",
    "    print(clean_string(test))\n",
    "else:\n",
    "    print('Not a list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_name(names):\n",
    "\n",
    "    if names == ('NULL' or ''):\n",
    "        return []\n",
    "    \n",
    "    elif is_list(names) is True:\n",
    "        return clean_string(names)\n",
    "        \n",
    "    else:\n",
    "        return [names]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_file(filename):\n",
    "    data = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        #skipping the extra metadata\n",
    "        for i in range(3):\n",
    "            l = next(reader)\n",
    "        # processing file\n",
    "        for line in reader:\n",
    "            # calling your function to fix the area value\n",
    "            if \"name\" in line:\n",
    "                #print('Old: ', line['name'])\n",
    "                line[\"name\"] = fix_name(line[\"name\"])\n",
    "                #print('New: ', line['name'], '\\n')\n",
    "            data.append(line)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    data = process_file(CITIES)\n",
    "\n",
    "    print('Printing 20 results:')\n",
    "    for n in range(20):\n",
    "        pprint.pprint(data[n][\"name\"])\n",
    "\n",
    "    assert data[14][\"name\"] == ['Negtemiut', 'Nightmute']\n",
    "    assert data[9][\"name\"] == ['Pell City Alabama']\n",
    "    assert data[3][\"name\"] == ['Kumhari']\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Top of document)\n",
    "<a id='quiz_crossfield_auditing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Crossfield Auditing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem set you work with cities infobox data, audit it, come up with a\n",
    "cleaning idea and then clean it up.\n",
    "\n",
    "If you look at the full city data, you will notice that there are couple of\n",
    "values that seem to provide the same information in different formats: \"point\"\n",
    "seems to be the combination of \"wgs84_pos#lat\" and \"wgs84_pos#long\". However,\n",
    "we do not know if that is the case and should check if they are equivalent.\n",
    "\n",
    "Finish the function check_loc(). It will recieve 3 strings: first, the combined\n",
    "value of \"point\" followed by the separate \"wgs84_pos#\" values. You have to\n",
    "extract the lat and long values from the \"point\" argument and compare them to\n",
    "the \"wgs84_pos# values, returning True or False.\n",
    "\n",
    "Note that you do not have to fix the values, only determine if they are\n",
    "consistent. To fix them in this case you would need more information. Feel free\n",
    "to discuss possible strategies for fixing this on the discussion forum.\n",
    "\n",
    "The rest of the code is just an example on how this function can be used.\n",
    "Changes to \"process_file\" function will not be taken into account for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pprint\n",
    "\n",
    "CITIES = 'cities.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_loc(point, lat, longi):\n",
    "    point = point.split(' ')\n",
    "\n",
    "    if point[0].strip() == lat and point[1].strip() == longi:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_file(filename):\n",
    "    data = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        #skipping the extra matadata\n",
    "        for i in range(3):\n",
    "            l = next(reader)\n",
    "        # processing file\n",
    "        for line in reader:\n",
    "            # calling your function to check the location\n",
    "            print('Old: {}: Point {} Lat {} Long {}'.format(line[\"name\"], line[\"point\"], line[\"wgs84_pos#lat\"], line[\"wgs84_pos#long\"]))\n",
    "            result = check_loc(line[\"point\"], line[\"wgs84_pos#lat\"], line[\"wgs84_pos#long\"])\n",
    "            if not result:\n",
    "                print('{}: {} != {} {}'.format(line[\"name\"], line[\"point\"], line[\"wgs84_pos#lat\"], line[\"wgs84_pos#long\"]))\n",
    "            data.append(line)\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_file(CITIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    assert check_loc(\"33.08 75.28\", \"33.08\", \"75.28\") == True\n",
    "    assert check_loc(\"44.57833333333333 -91.21833333333333\", \"44.5783\", \"-91.2183\") == False\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
